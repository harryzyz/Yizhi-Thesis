
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{iitthesis}
%\documentclass[draft]{iitthesis}

% Document Options:
%
% Note if you want to save paper when printing drafts,
% replace the above line by
%
%   \documentclass[draft]{iitthesis}
%
% See Help file for more about options.

\usepackage[dvips]{graphicx}    % This package is used for Figures
\usepackage{rotating}           % This package is used for landscape mode.
\usepackage{epsfig}
\usepackage{subfigure}          % These two packages, epsfig and subfigure, are used for creating subplots.
% Packages are explained in the Help document.
\usepackage{amsmath,amssymb,amsthm,mathtools,bbm,booktabs,array,tikz,pifont,comment,multirow,url,graphicx}
\usepackage{color}
\input FJHDef.tex

%Requires ApproxUnivariate_k.tex, univariate_integration_k.tex, ConesPaperSpikyquad.eps, ConesPaperFlukyquad.eps

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\INT}{INT}
\DeclareMathOperator{\APP}{APP}
\DeclareMathOperator{\lin}{lin}
\DeclareMathOperator{\up}{up}
\DeclareMathOperator{\lo}{lo}
\DeclareMathOperator{\fix}{fix}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\maxcost}{maxcost}
\DeclareMathOperator{\mincost}{mincost}
\newcommand{\herr}{\widehat{\err}}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{algo}{Algorithm}
\newtheorem{condit}{Condition}
%\newtheorem{assump}{Assumption}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\Fnorm}[1]{\abs{#1}_{\cf}}
\newcommand{\Ftnorm}[1]{\abs{#1}_{\tcf}}
\newcommand{\Gnorm}[1]{\norm[\cg]{#1}}
\newcommand{\flin}{f_{\text{\rm{lin}}}}

\begin{document}

%%% Declarations for Title Page %%%
\title{Title}
\author{Yizhi Zhang}
\degree{Doctor of Philosophy}
\dept{Applied Mathematics}
\date{Date}
\copyrightnoticetrue      % crate copyright page or not
%\coadvisortrue           % add co-advisor. activate it by removing % symbol to add co-advisor
\maketitle                % create title and copyright pages


\prelimpages         % Settings of preliminary pages are done with \prelimpages command


%%%  Acknowledgement %%%
\begin{acknowledgement}     % acknowledgement environment, this is optional
\par  Will be added once thesis is finished
% or \input{acknowledgement.tex} % you need a separate acknowledgement.tex file to include it.
\end{acknowledgement}


% Table of Contents
\tableofcontents
\clearpage

% List of Tables
\listoftables

\clearpage

%List of Figures
\listoffigures

\clearpage

%List of Symbols(optional)

\listofsymbols
 \SymbolDefinition{$\beta$}{List of symbols will be added later}

 \clearpage



%%% Abstract %%%
\begin{abstract}           % abstract environment, this is optional
\par Abstract will be included once all parts are finished
% Those algorithms will be firstly created based on the composite trapezoidal rule. It is the "simplest??"
% or \input{abstract.tex}  %you need a separate abstract.tex file to include it.
\end{abstract}


\textpages     % Settings of text-pages are done with \textpages command

% Chapters are created with \Chapter{title} command
\Chapter{INTRODUCTION}

Introduce existing algorithms for integration problems.

show drwabacks

1. Non-adaptive
2. adaptive no guarantees
3. maybe more

introduce 
\begin{equation}\label{integral}
    \text{INT}(f)=\int_{a}^{b}f(x)dx\in\reals.
\end{equation}
    


\clearpage



\Chapter{Problem Statement, Definition and Assumptions}
In the previous chapter, I introduced the problem that this thesis is going to be focus on, which is the univariate integration problem, $\text{INT}(f)=\int_{a}^{b}f(x)dx\in\reals$. To solve this problem, we need some adaptive integration algorithms constructed by the fixed cost building blocks such as composite trapezoidal rule and composite Simpson's rule. 

The composite trapezoidal rule using $n$ equally spaced intervals between $(a,b)$ can be defined as:
\begin{equation}\label{traprule}
  T(f,n)=\frac{b-a}{2n}\sum_{j=0}^{n}(f(u_{j})+f(u_{j+1})),
\end{equation}
where
\begin{equation}\label{upts}
u_j=a+\frac{j(b-a)}{n}, \qquad j=0, \ldots, n, \qquad n\in\mathbb{N}.
\end{equation}
In the meanwhile, the composite Simpson's rule using $3n$ equally spaced intervals between $(a,b)$ can be defined as:
\begin{equation}\label{simrule}
  S(f,n)=\frac{b-a}{6n}\sum_{j=0}^{3n}(f(v_{j})+4f(v_{j+1})+f(v_{j+2})),
\end{equation}
(\textcolor[rgb]{1.00,0.00,0.00}{double check index and fraction it is not right.})
where
\begin{equation}\label{vpts}
v_j=a+\frac{j(b-a)}{n}, \qquad j=0, \ldots, 3n, \qquad n\in 2\mathbb{N}.
\end{equation}
The reason why we use $3n$ intervals with an even number of $n$ is that...

In order to find the error bound of guaranteed algorithms, we start from some existing result of the error bound for trapezoidal rule and Simpson's rule in terms of the variation of the function. So we will have the following definition:

The space of input functions is $\cf:=\mathcal{V}^{1} \text{ and } \mathcal{V}^{3}$, separately for trapezoidal rule and Simpson's rule.  The general definitions of the variation, some relevant norms and spaces are as follows:
\begin{subequations} \label{defSobolev}
\begin{gather}
\Var(f) := \sup_{\substack{n \in \naturals\\ a = x_0 < x_1 < \cdots < x_{n} =b}} \sum_{i=1}^n \abs{f(x_i)-f(x_{i-1})}, \\
\norm[1]{f}:= \int_a^b \abs{f(x)} \, \dif x , \\
\cv^{k}: =\cv^{k}[a,b]=\{f\in C[a,b]: \Var(f^{(k)}) < \infty \}.
%\mathcal{W}^{k,p}=\mathcal{W}^{k,p}[0,1]=\{f\in C[0,1]: \|f^{(k)}\|_{p}<\infty\}.
\end{gather}
\end{subequations}

Define partition $u$ and $v$, or $x$? what should I use here? maybe $x$

For trapezoidale rule, we use $\widehat{V}_{1}(f',\{v_j\}_{j=0}^{n}$ to approximate $\Var(f')$, where %)=\norm[1]{f'-f(b)+f(a)}
\begin{align}\label{1direst}
\widehat{V}_{1}(f',\{v_j\}_{j=0}^{n})=\sum_{j=1}^{n}\left|f(v_{j+1})-f(v_{j}) - \frac{f(b)-f(a)}{n}\right|.
\end{align}
 
For Simpson's rule, we use $\widehat{V}(f''',\{u_j\}_{j=0}^{n+1})$ to approximate $\Var(f''')$, where
\begin{equation}\label{vhatlessvar}
    \widehat{V}_{3}(f''',\{v_j\}_{j=0}^{n+1})=\sum_{j=1}^{n-1}|f'''(u_{j+1})-f'''(u_{j})|.
\end{equation}

So the algorithm will be guaranteed to work for the cone of integrands for which $\widehat{V}(f^{(p)},\{x_j\}_{j=0}^{n+1})$ does not underestimate $\Var{(f^{(p)})}$ too much:
%\begin{equation}
\begin{multline}\label{coneinteg}
\cc:=\left\{f\in \cv^{p}, \Var(f^{(p)})\leq \mathfrak{C}(\text{size}(\{x_j\}_{j=0}^{n+1}))\widehat{V}_{p}(f^{(p)},\{x_j\}_{j=0}^{n+1}),\right.\\ \left.\text{for all choices of } n\in \mathbb{N}, \text{and }\{x_j\}_{j=0}^{n+1} \text{with }\text{size}(\{x_j\}_{j=0}^{n+1})<\mathfrak{h}\right\},
\end{multline}

the problem with $N$ and $2N$

The goal is to find an algorithm that can provide an upper bound of the approximation error using only function values. In the next chapter, I will give detailed deduction of error bound analysis.

\textcolor{red}{I need to write down more details. I need to give all definition before used in the algorithms for both trap and sim.}




\Chapter{Error Bound Analysis}
\textcolor{red}{I need to translate trap language to sim language to make them uniform. I need to explain the deduction for trap without using and assumptions or known theories in the paper. I also need to figure out notations. I need the notations not to conflict. Then I need to go to chapter 2 and change notation.}

\Section{Trapezoidal Rule}
From (ref), the error bound of Trapezoidal rule is related to the variation of the third derivatives of the function to be integrated:
\begin{equation}\label{errorboundtrap}
    \text{err}(f,n)\le\overline{\text{err}}(f,n):=\frac{(b-a)^2\Var(f')}{8n^2}.
\end{equation}

Note that $\tF_{n}(f)$ never overestimates $\Ftnorm{f}$ because
\begin{align*}
\Ftnorm{f} & = \bignorm[1]{f'-A_2(f)'}
= \sum_{i=1}^{n-1} \int_{x_i}^{x_{i+1}} \abs{f'(x) - A_2(f)'(x)} \, \dif x \\
& \ge \sum_{i=1}^{n-1} \abs{\int_{x_i}^{x_{i+1}} [f'(x) - A_2(f)'(x)] \, \dif x}=\norm[1]{A_n(f)'-A_2(f)'} = \tF_n(f).
\end{align*}
To find an upper bound on $\Ftnorm{f}-\tF_{n}(f)$, note that
\begin{equation*}
\Ftnorm{f} - \tF_{n}(f) = \Ftnorm{f} - \bigabs{A_n(f)}_{\tcf} \le \bigabs{f-A_n(f)}_{\tcf} = \bignorm[1]{f' -A_n(f)'},
\end{equation*}
since $(f-A_n(f))(x)$ vanishes for $x=0,1$.  Moreover,
\begin{equation} \label{onenormfp}
\bignorm[1]{f' -A_n(f)'} = \sum_{i=1}^{n-1} \int_{x_i}^{x_{i+1}} \abs{f'(x) -(n-1)[f(x_{i+1})-f(x_i)]} \, \dif x.
\end{equation}
Now we bound each integral in the summation.  For $i=1, \ldots, n-1$, let $\eta_i(x) = f'(x) -(n-1)[f(x_{i+1})-f(x_i)]$, and let $p_i$ denote the probability that $\eta_i(x)$ is non-negative:
\[
p_i = (n-1)\int_{x_i}^{x_{i+1}} \bbone_{[0,\infty)} (\eta_i(x)) \, \dif x,
\]
and so $1-p_i$ is the probability that $\eta_i(x)$ is negative.  Since $\int_{x_i}^{x_{i+1}} \eta_i(x) \, \dif x =0$, we know that $\eta_i$ must take on both non-positive and non-negative values.  Invoking the Mean Value Theorem, it follows that
\begin{multline*}
\frac{p_i}{n-1} \sup_{x_i \le x \le x_{i+1}} \eta_i(x) \ge \int_{x_i}^{x_{i+1}} \max(\eta_i(x),0) \, \dif x \\
= \int_{x_i}^{x_{i+1}} \max(-\eta_i(x),0) \, \dif x \le \frac{-(1-p_i)}{n-1} \inf_{x_i \le x \le x_{i+1}} \eta_i(x) .
\end{multline*}
These bounds allow us to derive bounds on the integrals in \eqref{onenormfp}:
\begin{align*}
\MoveEqLeft{\int_{x_i}^{x_{i+1}} \abs{\eta_i(x)} \, \dif x} \\
 &= \int_{x_i}^{x_{i+1}} \max(\eta_i(x),0) \, \dif x + \int_{x_i}^{x_{i+1}} \max(-\eta_i(x),0) \, \dif x\\
&=2(1-p_i) \int_{x_i}^{x_{i+1}} \max(\eta_i(x),0) \, \dif x + 2p_i\int_{x_i}^{x_{i+1}} \max(-\eta_i(x),0) \, \dif x\\
&\le \frac{2p_i(1-p_i)}{n-1} \left[ \sup_{x_i \le x \le x_{i+1}} \eta_i(x) - \inf_{x_i \le x \le x_{i+1}} \eta_i(x) \right]\\
&\le\frac{1}{2(n-1)} \left[ \sup_{x_i \le x \le x_{i+1}} f'(x) - \inf_{x_i \le x \le x_{i+1}} f'(x) \right],
\end{align*}
since $p_i(1-p_i)\le 1/4$.

Plugging this bound into \eqref{onenormfp} yields
\begin{align*}
\bignorm[1]{f'-f(1)+f(0)} - \tF_n(f) &= \Ftnorm{f} - \tF_{n}(f)\\
 & \le \bignorm[1]{f' -A_n(f)'}\\
&\le \frac{1}{2n-2} \sum_{i=1}^{n-1} \left[ \sup_{x_i \le x \le x_{i+1}} f'(x) - \inf_{x_i \le x \le x_{i+1}} f'(x) \right] \\
& \le \frac{\Var(f')}{2n-2} \leq \frac{\tau}{2n-2}\Ftnorm{f}  ,
\end{align*}

So $$\Ftnorm{f}\leq\frac{2n-2}{2n-2-\tau}\tF_n(f).$$

Then we will have $$\overline{\text{err}}(f,n):=\frac{(b-a)^2\Var(f')}{8n^2}\leq\frac{(b-a)\tau}{4(n-1)(2n-2-\tau)}\tF_n(f).$$

\Section{Simpson's Rule}

From (ref), the error bound of Simpson's rule is related to the variation of the third derivatives of the function to be integrated:
\begin{equation}\label{errorboundSimpson}
    \text{err}(f,n)\le\overline{\text{err}}(f,n):=\frac{(b-a)^4\Var(f''')}{5832n^4}.
\end{equation}

We do not have the variation of the third derivative of the function. In order to find the error bound, the approximation (1) and (2) of variation of the third derivative of the function defined in Chapter 2 became important.

%$\{t_i\}_{i=0}^{6n}$, where $a=t_{0}\le t_{1}\le\cdots\le t_{6n-1}\le t_{6n}=b$, and partition   and $t_{3j-3}\le x_{j}\le t_{3j}$ for $j=1,\cdots,2n$,
By definition, the approximation (1) is actually a lower bound of $\Var(f''')$:
\begin{equation}\label{vhatlessvar}
    \widehat{V}(f''',\{x_j\}_{j=0}^{n+1})\leq\Var{(f''')}, \quad \forall f \in \cc, \quad \{x_j\}_{j=0}^{n+1}, \quad n \in \mathbb{N}.
\end{equation}

The algorithm will be guaranteed to work for the cone (2) of integrands for which $\widehat{V}(f''',\{x_j\}_{j=0}^{n+1})$ does not underestimate $\Var{(f''')}$ too much.

%Approximation of $\Var(f''')$ using only function values:
We cannot use $\widehat{V}(f''',\{x_j\}_{j=0}^{n+1})$ to approximate $\Var{(f''')}$ because it depends on values of $f'''$, not values of $f$. However, $\widehat{V}(f''',\{x_j\}_{j=0}^{n+1})$ is closely related to the following approximation to $\Var{(f''')}$:
\begin{multline}\label{vtilde}
\widetilde{V}_n(f)=\frac{27n^3}{(b-a)^3}\sum_{j=1}^{n-1}\left|f(v_{3j+3})-3f(v_{3j+2})+3f(v_{3j+1})\right.\\\left.-2f(v_{3j})+3f(v_{3j-1})-3f(v_{3j-2})+f(v_{3j-3})\right|,
\end{multline}

% may be say more
% Since $$\frac{216n^3}{(b-a)^3}\left|f(t_{i-3})-3f(t_{i-2})+3f(t_{i-1})-f(t_{i})\right|=f'''(x_{i-1}),???$$ for some $x_{i-1} \in [t_{3i-3},t_{3i}]$,
We use divided differences to explain The relationship between (1) and (2). Let $h=v_{i+1}-v_{i}=(b-a)/3n$ and
\begin{align*}
  f[v_{i}]&=f(v_{i}), \text{ for } i=0,\cdots, 3n,\\
  f[v_{i},v_{i-1}]&=\frac{f(v_{i})-f(v_{i-1})}{h},\text{ for } i=1, \cdots, 3n,\\
  f[v_{i},v_{i-1},v_{i-2}]&=\frac{f(v_{i})-2f(v_{i-1})+f(v_{i-2})}{2h^2},\text{ for } i=2, \cdots, 3n,\\
  f[v_{i},v_{i-1},v_{i-2},v_{i-3}]&=\frac{f(v_{i})-3f(v_{i-1})+3f(v_{i-2})-f(v_{i-3})}{6h^3}, \text{ for } i=3, \cdots, 3n.
\end{align*}

According to Mean Value Theorem for divided differences, (ref), for all $j=1,2,\cdots,n$, $\exists x_j\in (v_{3j-3},v_{3j})$ such that
\begin{equation*}
    f[v_{3j},v_{3j-1},v_{3j-2},t_{3j-3}]=\frac{f'''(x_j)}{6},
\end{equation*}
for $j = 1, 2, \cdots, n.$ This implies that
\begin{multline}\label{vtileqftriprime}
  f'''(x_j)=\frac{f(v_{3j})-3f(v_{3j-1})+3f(v_{3j-2})-f(v_{3j-3})}{h^3},\\=\frac{27n^3}{(b-a)^3}[f(v_{3j})-3f(v_{3j-1})+3f(v_{3j-2})-f(v_{3j-3})].
\end{multline}



If we combine \eqref{vtilde} and \eqref{vtileqftriprime} together, we obtain
\begin{equation}\label{vtileqvhat}
    \widetilde{V}_n(f)=\sum_{j=1}^{n-1}\left|f'''(x_{j+1})-f'''(x_{j})\right|=\widehat{V}(f''',\{x_j\}_{j=0}^{n+1}).
\end{equation}
Then we can use $\widetilde{V}_n(f)$ to approximate $\Var(f''')$ by just using function values.


\Chapter{Adaptive, Automatic Algorithms with Guarantees}

\Section{Basic Concepts}
AA
\textcolor{red}{I need to explain the embedded mechanism and stopping criteria with tolerance, max iteration and max number of points. In the following subsections, I need detailed explanation of the algorithms.}


\Section{Trapezoidal Rule}


\begin{algo}[Adaptive Univariate Integration] \label{multistageintegalgo}
Let the sequence of algorithms $\{T_n\}_{n\in \mathcal{I}}$, $\{\tF_n\}_{n\in \mathcal{I}}$, and $\{F_n\}_{n\in \mathcal{I}}$ be as described above.
Let $\tau\ge2$ be the cone constant. Set $i=1$. Let $n_1=\lceil(\tau+1)/2\rceil+1$. For any error tolerance $\varepsilon$ and input function $f$, do the following:
\begin{description}
\item[Stage 1.\ Estimate {$\norm[1]{f'-f(1)+f(0)}$} and bound {$\Var(f')$}.] Compute $\tF_{n_i}(f)$ in \eqref{1direst} and $F_{n_i}(f)$ in \eqref{Fnormalg}.

\item[Stage 2. Check the necessary condition for $f \in \cc_{\tau}$.] Compute
    \begin{align*}
     \tau_{\min,n_i} =  \frac{F_{n_i}(f)}{\tF_{n_i}(f)+F_{n_i}(f)/(2n_i-2)}.
    \end{align*}
If $\tau \ge \tau_{\min,n_i}$, then go to stage 3.  Otherwise, set $\tau = 2\tau_{\min,n_i}$.  If $n_i \ge (\tau+1)/2$, then go to stage 3.  Otherwise, choose
$$
n_{i+1}=1+ (n_i-1)\left\lceil\frac{\tau+1}{2n_i-2}\right\rceil.
$$
Go to Stage 1.

\item[Stage 3. Check for convergence.] Check whether $n_i$ is large enough to satisfy the error tolerance, i.e.
    \begin{equation*}
     \tF_{n_i}(f) \le \frac{4\varepsilon(n_i-1)(2n_i-2 - \tau)}{\tau}.
    \end{equation*}
If this is true, then return $T_{n_i}(f)$ and terminate the algorithm.   If this is not true, choose
$$
n_{i+1}=1+ (n_i-1)\max\left\{2,\left\lceil\frac{1}{(n_i-1)}\sqrt{\frac{\tau \tF_{n_i}(f)}{8\varepsilon}}\right\rceil\right\}.
$$
Go to Stage 1.
\end{description}
\end{algo}



\Section{Simpson's Rule}

\begin{algo}[Adaptive Univariate Integration] \label{multistageintegalgosimpson}
Given an interval $[a,b]$, an inflation function, $\mathfrak{C}$, a positive key mesh size $\mathfrak{h}$, a positive error tolerance, $\varepsilon$, and a routine for generating values of the integrand, $f$, set $l=1$, and
$n_1=2(\lfloor (b-a)/\mathfrak{h}\rfloor+1)$.%, if $\lfloor(n_1/2)\rfloor\neq n_1/2$, $n_1=n_1+1$.
\begin{description}
\item[Stage 1] %Estimate {$\|f''-4[f(1)-2f(1/2)+f(0)]\|_1$} and bound {$\Var(f''')$}.]
Compute the error estimate $\widetilde{\text{err}}(f,n_l)$ according to \eqref{errorboundcone}.

\item[Stage 2] %Check the necessary condition for $f \in \cc_{\tau}$.]
If $\widetilde{\text{err}}(f,n_l)\le\varepsilon$, then return the Simpson's rule approximation $S_{n_l}(f)$ as the answer.

\item[Stage 3] %Check for convergence.]
Otherwise let $n_{l+1}=\max(2,m)\eta_{l}$, where
$$m=\min\{r\in\mathbb{N}:\eta(rn_l)\widetilde{V}_{n_{l}(f)}\le\varepsilon\}, \text{with } \eta(n):=\frac{(b-a)^4\mathfrak{C}(2(b-a)/n)}{5832n^4}.$$
increase $l$ by one, and go to 1.
\end{description}
\end{algo}

\begin{theorem}\label{thmSimpson}
    Algorithm \ref{multistageintegalgosimpson} is successful, i.e.,
    \begin{equation*}
      \left|\int_{a}^{b}f(x)dx-\texttt{integral}(f,a,b,\varepsilon)\right|\le\varepsilon, \qquad \forall f\in \cc.
    \end{equation*}
\end{theorem}


\Chapter{Computational Cost of Guaranteed Algorithms}
\Section{Traezoidale rule}
\begin{theorem} \label{multistageintegthm}
Let $\sigma >0$ be some fixed parameter, and let $\cb_{\sigma}=\{f \in  \mathcal{V}^{1} : \Var(f')\leq \sigma\}$. Let $T \in \ca(\cb_{\sigma}, \reals, \INT, \Lambda^{\std})$ be the non-adaptive trapezoidal rule defined by Algorithm \ref{nonadaptalgo}, and let $\varepsilon>0$ be the error tolerance. Then this algorithm succeeds for $f \in \cb_{\sigma}$, i.e., $\abs{\INT(f) - T(f,\varepsilon)} \le \varepsilon$, and the cost of this algorithm is $\left \lceil \sqrt{\sigma/(8\varepsilon)}\right \rceil + 1$, regardless of the size of $\Var(f')$.

Now let $T \in \ca(\cc_{\tau}, \reals, \INT, \Lambda^{\std})$ be the adaptive trapezoidal rule defined by Algorithm \ref{multistageintegalgo}, and let $\tau$, $n_1$, and $\varepsilon$ be as described there. Let $\cc_\tau$ be the cone of functions defined in \eqref{coneinteg}.  Then it follows that Algorithm \ref{multistageintegalgo} is successful for all functions in $\cc_{\tau}$,  i.e.,  $\abs{\INT(f) - T(f,\varepsilon)} \le \varepsilon$.  Moreover, the cost of this algorithm is bounded below and above as follows:
\begin{multline}
\max \left(\left \lceil\frac{\tau+1}{2} \right \rceil, \left \lceil \sqrt{\frac{ \Var(f')}{8\varepsilon}} \right \rceil \right) +1 \\
\le \max \left(\left \lceil\frac{\tau+1}{2} \right \rceil, \left \lceil \sqrt{\frac{\tau \norm[1]{f'-f(1)+f(0)}}{8\varepsilon}} \right \rceil \right) +1 \\
\le
\cost(T,f;\varepsilon) \\
\le \sqrt{\frac{\tau \norm[1]{f'-f(1)+f(0)}}{2\varepsilon}} + \tau + 4
\le \sqrt{\frac{\tau \Var(f') }{4\varepsilon}} + \tau + 4.
\end{multline}
The algorithm is computationally stable, meaning that the minimum and maximum costs for all integrands, $f$, with fixed $\norm[1]{f'-f(1)+f(0)}$ or $\Var(f')$ are an $\varepsilon$-independent constant of each other.
\end{theorem}

\Section{Simpson's rule}
\begin{theorem}\label{uppbndcost}
    Let $N(f,\varepsilon)$ denote the final number of $n_l$ in Stage 2 when the algorithm terminates. Then this number is bounded below and above in terms of the true, yet unknown, $\Var(f''')$.
    \begin{multline}\label{uppbndcostineq}
        \max\left(\left\lfloor\frac{2(b-a)}{\mathfrak{h}}\right\rfloor+1,\left\lceil(b-a)\left(\frac{\Var(f''')}{5832\varepsilon}\right)^{1/4}\right\rceil\right)\leq N(f,\varepsilon)\\ \leq 2\min\left\{n\in\mathbb{N}:n\geq2\left(\left\lfloor\frac{(b-a)}{\mathfrak{h}}\right\rfloor+1\right),\eta(n)\Var(f''')\leq\varepsilon\right\}\\ \leq 2\min_{0<\alpha\leq1}\max\left(2\left(\left\lfloor\frac{(b-a)}{\alpha\mathfrak{h}}\right\rfloor+1\right),(b-a)\left(\frac{\mathfrak{C}(\alpha\mathfrak{h})\Var(f''')}{5832\varepsilon}\right)^{1/4}+1\right).
    \end{multline}
    The number of function values required by the algorithm is $3N(f,\varepsilon)+1$.
\end{theorem}
\begin{proof}
  %No matter what inputs $f$ and $\varepsilon$ are provided, the number of intervals must be at least $n_1=\lfloor2(b-a)/\mathfrak{h}\rfloor+1$ in order to comply with both Simpson's rule and divided differences method. Then the number of intervals increases until $\widetilde{\text{err}}(f,n)\le\varepsilon$, which by \eqref{errorboundcone} implies that $\overline{\text{err}}(f,n)\le\varepsilon$. This implies the lower bound on $N(f,\varepsilon)$.
  No matter what inputs $f$ and $\varepsilon$ are provided, $N(f,\varepsilon)\ge n_1=2(\lfloor (b-a)/\mathfrak{h}\rfloor+1)$. Then the number of intervals increases until $\widetilde{\text{err}}(f,n)\le\varepsilon$, which by \eqref{errorboundcone} implies that $\overline{\text{err}}(f,n)\le\varepsilon$. This implies the lower bound on $N(f,\varepsilon)$.

  Let $L$ be the value of $l$ for which Algorithm \ref{multistageintegalgosimpson} terminates. Since $n_1$ satisfies the upper bound, we may assume that $L \ge 2$. Let $m$ be the integer found in Step 3, and let $m^*=\max(2,m)$. Note that $\eta((m^*-1)n_{L-1})\Var(f''')>\varepsilon$. For $m^*=2$, this is true because $\eta(n_{L-1})\Var(f''')\ge\eta(n_{L-1})\widetilde{V}_{n_{L-1}}(f)=\widetilde{\text{err}}(f,n_{L-1})>\varepsilon$. For $m^*=m>2$ it is true because of the definition of $m$. Since $\eta$ is a decreasing function, it follows that
  $$(m^*-1)n_{L-1}<n^*:=\min\left\{n\in\mathbb{N}:n\ge\left\lfloor\frac{2(b-a)}{n}\right\rfloor+1,\eta(n)\Var(f''')\le\varepsilon\right\}.$$
  Therefore $n_L=m^*n_{L-1}<m^*\frac{n^*}{m^*-1}=\frac{m^*}{m^*-1}n^*\le2n^*$.

  To prove the latter part of the upper bound, we need to prove that
  $$n^*\leq\max\left(\left\lfloor\frac{2(b-a)}{\alpha\mathfrak{h}}\right\rfloor+1,(b-a)\left(\frac{\mathfrak{C}(\alpha\mathfrak{h})\Var(f''')}{5832\varepsilon}\right)^{1/4}+1\right),\quad 0<\alpha<1.$$
  For fixed $\alpha\in(0,1]$, we only need to consider that case where $n^*>\left\lfloor2(b-a)/(\alpha\mathfrak{h})\right\rfloor+1$. This implies that $n^*-1>\left\lfloor2(b-a)/(\alpha\mathfrak{h})\right\rfloor\ge 2(b-a)/(\alpha\mathfrak{h})$ thus $\alpha\mathfrak{h}\ge2(b-a)/(n^*-1)$. Also by the definition of $n^*$, $\eta$, and $\mathfrak{C}$ is non-decreasing:
  \begin{align*}
    &\eta(n^*-1)\Var(f''')>\varepsilon, \\
    \Rightarrow 1&<\left(\frac{\eta(n^*-1)\Var(f''')}{\varepsilon}\right)^{1/4},\\
    \Rightarrow n^*-1&<n^*-1\left(\frac{\eta(n^*-1)\Var(f''')}{\varepsilon}\right)^{1/4},\\
    &=n^*-1\left(\frac{(b-a)^4\mathfrak{C}(2(b-a)/(n^*-1))\Var(f''')}{5832(n^*-1)^4\varepsilon}\right)^{1/4},\\
    &\le(b-a)\left(\frac{\mathfrak{C}(\alpha\mathfrak{h})\Var(f''')}{5832\varepsilon}\right)^{1/4}.
  \end{align*}
  This completes the prove of latter part of the upper bound.
\end{proof}


\Chapter{Lower Bound of Complexity}

\Section{Traezoidale rule}
Next, we derive a lower bound on the cost of approximating functions in the ball $\cb_{\sigma}$ and in the cone $\cc_{\tau}$ by constructing fooling functions. Following the arguments of Section \ref{LowBoundSec}, we choose  the triangle shaped function $f_0: x \mapsto 1/2-\abs{1/2-x}$. Then
\begin{gather*}
\Ftnorm{f_0}=\norm[1]{f'_0-f_0(1)+f_0(0)}=\int_0^1 \abs{\sign(1/2-x)} \, \dif x = 1, \\ \Fnorm{f_0}=\Var(f'_0)=2= \tau_{\min}.
\end{gather*}
For any $n \in \cj:=\natzero$, suppose that the one has the data $L_i(f)=f(\xi_i)$, $i=1, \ldots, n$ for arbitrary $\xi_i$, where $0=\xi_0 \le \xi_1 < \cdots < \xi_n \le \xi_{n+1} = 1$.  There must be some $j=0, \ldots, n$ such that $\xi_{j+1} - \xi_j \ge 1/(n+1)$.  The function $f_{1}$ is defined as a triangle function on the interval $[\xi_j, \xi_{j+1}]$:
$$
f_{1}(x):=\begin{cases} \displaystyle
\frac{\xi_{j+1}-\xi_{j}-\abs{\xi_{j+1}+\xi_{j}-2x}}{8} & \xi_{j} \le x \leq \xi_{j+1},\\
0 & \text{otherwise}.
\end{cases}
$$
This is a piecewise linear function whose derivative changes from $0$ to $1/4$ to $-1/4$ to $0$ provided $0 < \xi_j < \xi_{j+1} < 1$, and so $\Fnorm{f_1}=\Var(f'_1)\le 1$. Moreover,
\begin{gather*}
\INT(f)=\int_0^1 f_1(x) \, \dif x = \frac{(\xi_{j+1} - \xi_j)^2}{16} \ge \frac{1}{16(n+1)^2} =: g(n),\\
g^{-1}(\varepsilon)=\left \lceil \sqrt{\frac{1}{16 \varepsilon}} \right \rceil - 1.
\end{gather*}
Using these choices of $f_0$ and $f_1$, along with the corresponding $g$ above, one may invoke Theorems \ref{complowbdball}--\ref{complowbd}, and Corollary \ref{optimcor} to obtain the following theorem.

\begin{theorem} \label{complowbdinteg} For $\sigma>0$ let $\cb_{\sigma}=\{f \in \cv^{1} : \Var(f') \le \sigma\}$.  The complexity of integration on this ball is bounded below as
\begin{equation*}
\comp(\varepsilon,\ca(\cb_{\sigma},\reals,\INT,\Lambda^{\std}),\cb_{s}) \ge \left \lceil \sqrt{\frac{\min(s,\sigma)}{16 \varepsilon}} \right \rceil -1 .
\end{equation*}
Algorithm \ref{nonadaptalgo} using the trapezoidal rule has optimal order in the sense of Theorem \ref{optimalprop}.

For $\tau>2$, the complexity of the integration problem over the cone of functions $\cc_{\tau}$ defined in \eqref{coneinteg} is bounded below as
\begin{equation*}
\comp(\varepsilon,\ca(\cc_{\tau},\reals,\INT,\Lambda^{\std}),\cb_{s}) \ge \left \lceil \sqrt{\frac{(\tau-2)s}{32 \tau \varepsilon}} \right \rceil -1 .
\end{equation*}
The adaptive trapezoidal Algorithm \ref{multistageintegalgo} has optimal order for integration of functions in $\cc_{\tau}$ in the sense of Corollary \ref{optimcor}.
\end{theorem}

\Section{Simpson's rule}
building fooling function:
\begin{subequations} \label{bumpfunction}
%\begin{gather}
%bump(x;t,h):= \begin{cases} \displaystyle (x-t)^3/6, & t \le x < t+h,\\[1ex]
%\displaystyle [(x-t)^2(t+2h-x)+(x-t)(t+3h-x)(x-t-h)+(t+4h-x)(x-t-h)^2]/6, & t+h \le x < t+2h,\\[1ex]
%\displaystyle [(x-t)(t+3h-x)^2+(t+4h-x)(x-t-h)(t+3h-x)+(t+4h-x)^2(x-t-2h)]/6, & t+2h \le x < t+3h,\\[1ex]
%\displaystyle (t+4h-x)^3/6, & t+3h \le x < t+4h,\\[1ex]
%\displaystyle  0, & \text{otherwise},
%\end{cases}
%\\
\begin{gather}
\text{bump}(x;t,h):= \begin{cases} \displaystyle (x-t)^3/6, & t \le x < t+h,\\[1ex]
\displaystyle [-3(x-t)^3+12h(x-t)^2-12h^2(x-t)+4h^3]/6, & t+h \le x < t+2h,\\[1ex]
\displaystyle [3(x-t)^3-24h(x-t)^2+60h^2(x-t)-44h^3]/6, & t+2h \le x < t+3h,\\[1ex]
\displaystyle (t+4h-x)^3/6, & t+3h \le x < t+4h,\\[1ex]
\displaystyle  0, & \text{otherwise},
\end{cases}
\\
\text{bump}'''(x;t,h):= \begin{cases} \displaystyle 1, & t \le x < t+h,\\[1ex]
\displaystyle -3, & t+h \le x < t+2h,\\[1ex]
\displaystyle 3, & t+2h \le x < t+3h,\\[1ex]
\displaystyle -1, & t+3h \le x < t+4h,\\[1ex]
\displaystyle  0, & \text{otherwise},
\end{cases}, \\
\Var(\text{bump}'''(\cdot;t,h))\le 16 \text{ with equality if } a<t<t+4h<b, \\
\int_{a}^{b}\text{peak}(x;t,h)dx=h^4.
\end{gather}
\end{subequations}

The following double-bump function always lies in $\cc$:
\begin{subequations}
    \begin{multline}\label{foolingfunction}
        \text{twobp}(x;t,h,\pm):=\text{bump}(x;a,\mathfrak{h})\pm\frac{15[\mathfrak{C}(h)-1]}{16}\text{bump}(x;t,h)\\ a+5\mathfrak{h}\le h \le b-5h, 0\le h <\mathfrak{h}.
    \end{multline}
    \\
    \begin{equation}
        \Var(\text{twobp}'''(x;t,h,\pm))=15+16\frac{15[\mathfrak{C}(h)-1]}{16}=15\mathfrak{C}(h).
    \end{equation}
\end{subequations}
From this definition it follows that
\begin{align*}
  &\mathfrak{C}(\text{size}(\{x_j\}_{j=0}^{n+1}))\widehat{V}(\text{twobp}'''(x;t,h,\pm),\{x_j\}_{j=0}^{n+1})\\
  \ge & \begin{cases} \displaystyle 15\mathfrak{C}(h)=\Var(\text{twobp}'''(x;t,h,\pm)), h \le \text{size}(\{x_j\}_{j=0}^{n+1})) <\mathfrak{h}\\[1ex]
                      \displaystyle \mathfrak{C}(0)\Var(\text{twobp}'''(x;t,h,\pm)), 0\le \text{size}(\{x_j\}_{j=0}^{n+1}))<h
        \end{cases}\\
  \ge & \Var(\text{twobp}'''(x;t,h,\pm))
\end{align*}

Although $\text{twobp}'''(x;t,h,\pm)$ may have a bump with arbitrarily small width $4h$, the height is small enough for $\text{twobp}'''(x;t,h,\pm)$ to lie in the cone.


complexity:

\begin{theorem}\label{lowbndcost}
    Let $int$ be any (possibly adaptive) algorithm that succeeds for all integrands in $\cc$, and only uses function values. For any error tolerance $\varepsilon > 0$ and any arbitrary value of $\Var(f''')$, there will be some $f\in \cc$ for which $int$ must use at least
    \begin{equation}\label{lowbndcostineq}
        -\frac{5}{4}+\frac{b-a-5\mathfrak{h}}{8}\left[\frac{[\mathfrak{C}(0)-1]\Var( f''')}{\varepsilon}\right]^{1/4}
    \end{equation}
    function values. As $\Var(f''')/\varepsilon \rightarrow \infty$ the asymptotic rate of increase is the same as the computational cost of \texttt{integral}.
\end{theorem}
\begin{proof}
  For any positive $\alpha$, suppose that $\texttt{int}(\cdot,a,b,\varepsilon)$ evaluates integrand $\alpha\text{bump}'''(\cdot;t,h)$ at $n$ nodes before returning to an answer. Let $\{x_j\}_{j=1}^{m})$ be the $m<n$ ordered nodes used by $\texttt{int}(\cdot,a,b,\varepsilon)$ that fall in the interval $(x_{0},x_{m+1})$ where $x_{0}:=a+3\mathfrak{h}$, $x_{m+1}:=b-h$ (why $h$ but not $\mathfrak{h}$ or $5h$?) and $h:=(b-a-5\mathfrak{h})/(4n+5)$. There must e at least one of these $x_{j}$ with $i=0,\cdots,m$ for which
  \begin{align*}
    \frac{x_{j+1}-x_{j}}{4}\ge\frac{x_{m+1}-x_{0}}{4(m+1)}\ge\frac{x_{m+1}-x_{0}}{4(n+1)}=\frac{b-a-5\mathfrak{h}-h}{4n+4}=h.
  \end{align*}
  Choose one such $x_{j}$ and call it $t$. The choice of $t$ and $h$ ensures that $\texttt{int}(\cdot,a,b,\varepsilon)$ cannot distinguish between $\alpha\text{bump}(\cdot;t,h)$ and $\alpha\text{twobp}(\cdot;t,h,\pm)$. Thus
  \begin{align*}
    \texttt{int}(\alpha\text{twobp}(\cdot;t,h,\pm),a,b,\varepsilon)=\texttt{int}(\alpha\text{bump}(\cdot;t,h),a,b,\varepsilon)
  \end{align*}
  Moreover, $\alpha\text{bump}(\cdot;t,h)$ and $\alpha\text{twobp}(\cdot;t,h,\pm)$ are all in the cone $\cc$. This means that $\texttt{int}$ is successful for all of the functions.
  \begin{subequations}
  \begin{multline*}
    \varepsilon\ge\frac{1}{2}\left[\right.\left|\int_{a}^{b}\alpha\text{twobp}(x;t,h,-)dx-\texttt{int}(\alpha\text{twobp}(\cdot;t,h,-),a,b,\varepsilon)\right|\\
    +\left|\int_{a}^{b}\alpha\text{twobp}(x;t,h,+)dx-\texttt{int}(\alpha\text{twobp}(\cdot;t,h,+),a,b,\varepsilon)\right|\left.\right]
  \end{multline*}
  \begin{multline*}
    \ge\frac{1}{2}\left[\right.\left|\texttt{int}(\alpha\text{bump}(\cdot;t,h,-),a,b,\varepsilon)-\int_{a}^{b}\alpha\text{twobp}(x;t,h,-)dx\right|\\
    +\left|\int_{a}^{b}\alpha\text{twobp}(x;t,h,+)dx-\texttt{int}(\alpha\text{bump}(\cdot;t,h,+),a,b,\varepsilon)\right|\left.\right]
  \end{multline*}
  \begin{align*}
     &\ge\frac{1}{2}\left|\int_{a}^{b}\alpha\text{twobp}(x;t,h,+)dx-\int_{a}^{b}\alpha\text{twobp}(x;t,h,-)dx\right|\\
     &=\int_{a}^{b}\alpha\texttt{bump}(x;t,h)dx\\
     &=\frac{15\alpha[\mathfrak{C}(h)-1]h^4}{16}\\
     &=\frac{[\mathfrak{C}(h)-1]h^4\Var(\alpha\texttt{bump}'''(\cdot;a,\mathfrak{h}))}{16}
  \end{align*}
  \end{subequations}
  Substituting $h$  in terms of $n$:
      \begin{align*}
        4n+5=\frac{b-a-5\mathfrak{h}}{h}&\ge(b-a-5\mathfrak{h})\left[\frac{[\mathfrak{C}(h)-1]\Var(\alpha \texttt{bump}'''(\cdot;a,\mathfrak{h})))}{16\varepsilon}\right]^{1/4},\\
        &\ge\frac{b-a-5\mathfrak{h}}{2}\left[\frac{[\mathfrak{C}(0)-1]\Var(\alpha \texttt{bump}'''(\cdot;a,\mathfrak{h}))}{\varepsilon}\right]^{1/4}.
    \end{align*}
    Since $\alpha$ is an arbitrary positive number, the value of $\Var(\alpha \texttt{bump}'''(\cdot;a,\mathfrak{h}))$ is arbitrary.

    Finally, comparing the upper bound on the computational cost of $\texttt{integral}$ in \eqref{uppbndcostineq} with the lower bound on the computational cost of the best algorithm in \eqref{lowbndcostineq}, both of them increase as $\mathcal{O}((\Var(f''')/\varepsilon))^{1/4}$ as $(\Var(f''')/\varepsilon)^{1/4}\rightarrow \infty$. Thus $\texttt{integral}$ is optimal.
\end{proof}


\Chapter{Numerical Experiments}


\Section{Traezoidale rule}
Consider the family of bump test functions defined by
\begin{multline}\label{testfun}
f(x)= \\
\begin{cases}
\displaystyle  b[4a^2 + (x-z)^2 + (x-z-a)|x-z-a|\\
\qquad \qquad -(x-z+a)|x-z+a|], & z-2a\leq x\leq z+2a,\\[2ex]
\displaystyle  0, & \text{otherwise}.
\end{cases}
\end{multline}
with  $\log_{10}(a) \sim \cu[-4,-1]$, $z \sim \cu[2a,1-2a]$, and $b=1/(4a^3)$ chosen to make $\int_0^1 f(x) \, \dif x = 1$.  It follows that $\norm[1]{f'-f(1)+f(0)}=1/a$ and $\Var(f')=2/a^2$.  The probability that $f \in \cc_{\tau}$ is $\min\left(1,\max(0,\left(\log_{10}(\tau/2)-1\right)/3)\right).$

As an experiment, we chose $10000$ random test functions and applied Algorithm \ref{multistageintegalgo} with an error tolerance of  $\varepsilon = 10^{-8}$ and initial $\tau$ values of $10, 100, 1000$.  The algorithm is considered successful for a particular $f$ if the exact and approximate integrals agree to within $\varepsilon$. The success and failure rates are given in Table \ref{integresultstable}. Our algorithm imposes a cost budget of $N_{\max}=10^7$.  If the proposed $n_{i+1}$ in Stages 2 or 3 exceeds $N_{\max}$, then our algorithm returns a warning and falls back to the largest possible $n_{i+1}$ not exceeding $N_{\max}$ for which $n_{i+1}-1$ is a multiple of $n_i-1$.  The probability that $f$ initially lies in $\cc_{\tau}$ is the smaller number in the third column of Table \ref{integresultstable}, while the larger number is the empirical probability that $f$ eventually lies in $\cc_{\tau}$ after possible increases in $\tau$ made by Stage 2 of Algorithm \ref{multistageintegalgo}.  For this experiment Algorithm \ref{multistageintegalgo} was successful for all $f$ that finally lie inside $\cc_{\tau}$ and for which no attempt was made to exceed the cost budget.

\begin{table}[h]
\centering
\begin{tabular}{cccccc}
&&&Success & Success & Failure \\
& $\tau$ &  $\Prob(f \in \cc_{\tau}) $ & No Warning & Warning & No Warning \\
\toprule
&$10$ & $0\% \rightarrow  25\% $ & $25\%$ & $<1\%$ & $75\%$  \\
Algorithm \ref{multistageintegalgo}
 &$100$ & $23 \% \rightarrow 58\% $ & $56\%$ & $2\%$ & $42\%$ \\
&$1000$ & $57\% \rightarrow 88\% $& $68\%$ & $20\%$ &$12\%$ \\
\midrule
{\tt quad} & & & 8\% & & $92\%$\\
{\tt integral} & & & 19\% & & $81\%$\\
{\tt chebfun} & & &29\% & & $71\%$\\
\end{tabular}
\caption{The probability of the test function lying in the cone for the original and eventual values of $\tau$ and the empirical success rate of Algorithm \ref{multistageintegalgo} plus the success rates of other common quadrature algorithms. \label{integresultstable}}
\end{table}

Some commonly available numerical algorithms in MATLAB are {\tt quad} and {\tt integral} \cite{MAT8.1} and the MATLAB Chebfun toolbox \cite{TrefEtal12}. We applied these three routines to the random family of test functions.  Their success and failure rates are also recorded in Table \ref{integresultstable}.  They do not give warnings of possible failure.

\Section{Simpson's Rule}
\textcolor{red}{I need to use a good example to run the tests. Then I will need to compare the results for both algorithms with other algorithms. After that, I need to compare trap and sim to get the conclusion such as sim has faster convergence rate than trap. This will require an sample function good to both trap and sim.}



\Chapter{CONCLUSION}
 %   \input{Conclusion.tex}

A


\Section{Summary}

A

\clearpage


%
% APPENDIX
%

% Do the settings of appendices with \appendix command
\appendix

% Then create each appendix using
% \Appendix{title_of_appendix} command

\Appendix{Table of Transition Coefficients for the Design of
Linear-Phase FIR Filters}

Your Appendix will go here !

% \moretox

  \Appendix{Name of your Second
Appendix}

Your second appendix text....

\Appendix{Name of your Third Appendix}

Your third appendix text....
%
% BIBLIOGRAPHY
%
% you have two options: 1) create bibliography manually,
% 2) create bibliography automatically. See BibliographyHelp.pdf file for details.

%
%\bibliographystyle{plain}
%\bibliography{mybib}

\end{document}  % end of document
